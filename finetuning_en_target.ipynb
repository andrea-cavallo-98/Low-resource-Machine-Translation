{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyi2y3sPAtep"
      },
      "source": [
        "# Get ALT dataset\n",
        "Download ALT dataset (the documentation can be found at [this link](https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/)), containing translations to different Asian languages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcdzn4BrAC7Z"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!wget https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/ALT-Parallel-Corpus-20191206.zip\n",
        "# Unzip dataset\n",
        "!unzip ALT-Parallel-Corpus-20191206.zip\n",
        "! rm ALT-Parallel-Corpus-20191206.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJUoVDBntLMj",
        "outputId": "dfd85be8-67bb-4493-e19c-796d106e6067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3x2IGBI7DTB"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQC7ka5H7B94"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets metrics sacrebleu transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OA__dfbokdb"
      },
      "source": [
        "Start from pretrained MarianMT from English to Chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAcbmsuJ7yhl"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-zh\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDETaqFWor4C"
      },
      "source": [
        "Select language and corresponding tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcO7g9hga2UP"
      },
      "outputs": [],
      "source": [
        "language = 'vi'\n",
        "\n",
        "if language == 'vi': # Vietnamese\n",
        "  mbart_language = \"vi_VN\"\n",
        "elif language == 'indo': # Indonesian\n",
        "  mbart_language = \"id_ID\"\n",
        "elif language == 'fil': # Filipino\n",
        "  mbart_language = 'tl_XX'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKXcmZ8Y727a"
      },
      "outputs": [],
      "source": [
        "# Load english and target language from dataset\n",
        "import pandas as pd\n",
        "\n",
        "df_en = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_en.txt\", sep='\\t', header=None, names=[\"id\", \"en\"])\n",
        "df_target = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_\" + language + \".txt\", sep='\\t', header=None, names=[\"id\", language])\n",
        "df_en = df_en.set_index(\"id\")\n",
        "df_target = df_target.set_index(\"id\")\n",
        "df_en_target = df_en.join(df_target)\n",
        "df_en_target.dropna(inplace=True)\n",
        "df_en_target.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lwnk0dQzh5T"
      },
      "outputs": [],
      "source": [
        "### \n",
        "# Split dataset into training, evaluation and test\n",
        "###\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "## For mixed finetuning, add also Chinese sentences to the training dataset\n",
        "\n",
        "TRAINING_TYPE = \"pure-finetuning\"\n",
        "#TRAINING_TYPE = \"mixed-finetuning\"\n",
        "\n",
        "if TRAINING_TYPE == \"pure-finetuning\":\n",
        "\n",
        "  ## Add special token to target language dataset (\"<2zz>\" for language zz)\n",
        "  df_en_target[\"en\"] = \"<2\" + language + \"> \" + df_en_target[\"en\"]\n",
        "\n",
        "  train_df, test_df = train_test_split(df_en_target, test_size=0.2, random_state=42)\n",
        "  eval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
        "\n",
        "elif TRAINING_TYPE == \"mixed-finetuning\": # add also Chinese sentences\n",
        "\n",
        "  ## Add special token to target language dataset (\"<2zz>\" for language zz)\n",
        "  df_en_target[\"en\"] = \"<2\" + language + \"> \" + df_en_target[\"en\"]\n",
        "\n",
        "  # evaluation and test sets only have target language, training set also has Chinese \n",
        "  train_df, test_df = train_test_split(df_en_target, test_size=0.2, random_state=42)\n",
        "  eval_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
        "\n",
        "  # Add Chinese sentences to the training set\n",
        "  df_en = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_en.txt\", sep='\\t', header=None, names=[\"id\", \"en\"])\n",
        "  df_zh = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_zh.txt\", sep='\\t', header=None, names=[\"id\", language])\n",
        "  df_en = df_en.set_index(\"id\")\n",
        "  df_zh = df_zh.set_index(\"id\")\n",
        "  df_en_zh = df_en.join(df_zh)\n",
        "  df_en_zh.dropna(inplace=True)\n",
        "  \n",
        "  ## Add special token to Chinese dataset\n",
        "  df_en_zh[\"en\"] = \"<2zh> \" + df_en_zh[\"en\"]\n",
        "\n",
        "  # Select subset of Chinese dataset to balance it with target language\n",
        "  df_en_zh_train, _ = train_test_split(df_en_zh, test_size=0.2, random_state=21)\n",
        "  train_df = pd.concat([train_df[[\"en\", language]], df_en_zh_train[[\"en\", language]]])\n",
        "\n",
        "  print(\"Vietnamese dataset size: \", df_en_target.shape)\n",
        "  print(\"Chinese dataset size: \", df_en_zh.shape)\n",
        "  print(\"Vietnamese + Chinese dataset size: \", train_df.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwgfO6_bcA5"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Load tokenizers: basic Marian tokenizer + pretrained mBart tokenizer\n",
        "####\n",
        "\n",
        "from transformers import AutoTokenizer, MBart50TokenizerFast, MarianTokenizer\n",
        "import random, transformers\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "\n",
        "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=mbart_language)\n",
        "marian_tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ_GOeMVtxR0"
      },
      "outputs": [],
      "source": [
        "# Check how many sentences are longer than the length limit\n",
        "\n",
        "num_tokens = 128\n",
        "\n",
        "tokenized_en_sentences = mbart_tokenizer(list(df_en_target[\"en\"]))[\"input_ids\"]\n",
        "longer_en = sum([len(s) > num_tokens for s in tokenized_en_sentences])\n",
        "print(f\"Out of {df_en_target.shape[0]} English sentences, {longer_en} ({longer_en/df_en_target.shape[0] * 100}%) have more than {num_tokens} tokens\")\n",
        "\n",
        "tokenized_vi_sentences = mbart_tokenizer(list(df_en_target[\"vi\"]))[\"input_ids\"]\n",
        "longer_vi = sum([len(s) > num_tokens for s in tokenized_vi_sentences])\n",
        "print(f\"Out of {df_en_target.shape[0]} Vietnamese sentences, {longer_vi} ({longer_vi/df_en_target.shape[0] * 100}%) have more than {num_tokens} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFrFyhQBbpsb"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Extend Marian tokenizer by adding tokens from target language according\n",
        "# to how mBart tokenizes sentences in the dataset\n",
        "####\n",
        "\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "tokensZoo = []\n",
        "\n",
        "for sentence in tqdm(list(df_en_target[language])):\n",
        "  tokenized_sentence = mbart_tokenizer(sentence)\n",
        "  for t in mbart_tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"]):\n",
        "    if t.lstrip(\"▁\") not in tokensZoo and t.lstrip(\"▁\") != \"\":\n",
        "      tokensZoo.append(t.lstrip(\"▁\"))\n",
        "    \n",
        "\n",
        "print(f\"{len(tokensZoo)} tokens to be added.\")\n",
        "print(f\"initial vocab size: {len(marian_tokenizer)}\")\n",
        "initial_len = len(marian_tokenizer)\n",
        "marian_tokenizer.add_tokens(tokensZoo, special_tokens=True)\n",
        "marian_tokenizer.add_tokens([\"<2zh>\", \"<2\"+language+\">\"], special_tokens=True)\n",
        "print(f\"final vocab size: {len(marian_tokenizer)}\")\n",
        "added_tokens = len(marian_tokenizer) - initial_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OOetIhRNoBq"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Tokenize the sentences in the dataset\n",
        "####\n",
        "\n",
        "from transformers import AutoTokenizer, MBart50TokenizerFast, MarianTokenizer\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import transformers    \n",
        "\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "# create dataset objects\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "eval_ds = Dataset.from_pandas(eval_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "\n",
        "max_input_length = 64\n",
        "max_target_length = 64\n",
        "batch_size = 16\n",
        "\n",
        "# To tokenize English, use a basic Marian tokenizer (the extended version\n",
        "# has some problems when tokenizing English)\n",
        "pure_marian_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "pure_marian_tokenizer.add_tokens([\"<2zh>\", \"<2\"+language+\">\"], special_tokens=True)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    targets = [s for s in examples[language]]\n",
        "    inputs = [s for s in examples[\"en\"]]\n",
        "\n",
        "    model_inputs = pure_marian_tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "    # Setup the tokenizer for targets\n",
        "    with marian_tokenizer.as_target_tokenizer():\n",
        "        labels = marian_tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    if (len(model_inputs[\"input_ids\"][0])!=len(model_inputs[\"input_ids\"][1])):\n",
        "        print (\"Error!\", )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "train_ds = train_ds.map(preprocess_function, batched=True, batch_size=batch_size)\n",
        "train_ds.set_format(type='torch', columns=columns_to_return)\n",
        "eval_ds = eval_ds.map(preprocess_function, batched=True, batch_size=batch_size)\n",
        "eval_ds.set_format(type='torch', columns=columns_to_return)\n",
        "test_ds = test_ds.map(preprocess_function, batched=True, batch_size=batch_size)\n",
        "test_ds.set_format(type='torch', columns=columns_to_return)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhIdjF_L59ju"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Define the function to compute the BLEU score during training\n",
        "####\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = marian_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, marian_tokenizer.pad_token_id)\n",
        "    decoded_labels = marian_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    print(\"Decoded preds: \", decoded_preds[0:3])\n",
        "    print(\"Decoded labels: \", decoded_labels[0:3])\n",
        "\n",
        "    if language == 'zh':\n",
        "      result = metric.compute(predictions=decoded_preds, references=decoded_labels, tokenize='zh')\n",
        "    else:\n",
        "      result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != marian_tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpKipSeXbiGn"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Download the initial translation model\n",
        "####\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "USE_PRETRAINED = True\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/mixed_ckp_final\")\n",
        "else:\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "  model.resize_token_embeddings(len(marian_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lYJmo06yqYE"
      },
      "source": [
        "# Define training parameters and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTuB-IGrV6nI"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Define training arguments\n",
        "###\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "  \"/content/drive/MyDrive/mixed_ckp\",\n",
        "  evaluation_strategy = \"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  per_device_eval_batch_size=batch_size,\n",
        "  weight_decay=0.01,\n",
        "  save_total_limit=1,\n",
        "  num_train_epochs=5,\n",
        "  predict_with_generate=True,\n",
        "  fp16=True, #CUDA purposes,\n",
        "  disable_tqdm=False,\n",
        "  #resume_from_checkpoint = \"final_checkpoint\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(marian_tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=marian_tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqQZo2_eY3jL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/pure_ckp_final\")"
      ],
      "metadata": {
        "id": "ftXqyLcjYd3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainer.predict(test_ds))"
      ],
      "metadata": {
        "id": "yzXJ99fd45nM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Low_resource_MT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}