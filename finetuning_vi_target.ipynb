{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyi2y3sPAtep"
      },
      "source": [
        "# Get ALT dataset\n",
        "Download ALT dataset (the documentation can be found at [this link](https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/)), containing translations to different Asian languages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcdzn4BrAC7Z"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!wget https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/ALT-Parallel-Corpus-20191206.zip\n",
        "# Unzip dataset\n",
        "!unzip ALT-Parallel-Corpus-20191206.zip\n",
        "! rm ALT-Parallel-Corpus-20191206.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminary steps"
      ],
      "metadata": {
        "id": "h2GILajOrWJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Define folders containing the pretrained models\n",
        "###\n",
        "\n",
        "checkpoint_vi_en = \"/content/checkpoint-9950\"\n",
        "checkpoint_en_indo = \"CLAck/indo_zh_indo\""
      ],
      "metadata": {
        "id": "ZOMGrqXorolA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJUoVDBntLMj",
        "outputId": "94887ce7-88b5-41d8-a803-2d9a82411c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQC7ka5H7B94"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets metrics sacrebleu transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDETaqFWor4C"
      },
      "source": [
        "# Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHANGE the name in /content/ALT-Parallel-Corpus-20191206/ of data_id.txt to data_indo.txt \n",
        "import os\n",
        "os.rename('/content/ALT-Parallel-Corpus-20191206/data_id.txt', '/content/ALT-Parallel-Corpus-20191206/data_indo.txt')"
      ],
      "metadata": {
        "id": "Zkq5uZ5NH7lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKXcmZ8Y727a"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Create dataframe with english, indonesian and vietnamese\n",
        "###\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_en = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_en.txt\", sep='\\t', header=None, names=[\"id\", \"en\"])\n",
        "df_vi = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_vi.txt\", sep='\\t', header=None, names=[\"id\", \"vi\"])\n",
        "df_indo = pd.read_csv(\"/content/ALT-Parallel-Corpus-20191206/data_indo.txt\", sep='\\t', header=None, names=[\"id\", \"indo\"])\n",
        "df_en = df_en.set_index(\"id\")\n",
        "df_vi = df_vi.set_index(\"id\")\n",
        "df_indo = df_indo.set_index(\"id\")\n",
        "\n",
        "# These will be useful for the tokenizer\n",
        "df_en_vi = df_en.join(df_vi)\n",
        "df_en_vi.dropna(inplace=True)\n",
        "df_en_indo = df_en.join(df_indo)\n",
        "df_en_indo.dropna(inplace=True)\n",
        "\n",
        "# This will be used for evaluation\n",
        "df_en_vi_indo = df_en_vi.join(df_indo)\n",
        "df_en_vi_indo.dropna(inplace=True)\n",
        "df_en_vi_indo.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### \n",
        "# Split dataset into training, evaluation and test\n",
        "# To make sure that test sentences have not been seen by the models during training,\n",
        "# the exact splits used for training are replicated, and are then merged together\n",
        "###\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Indonesian\n",
        "train_df, test_df_en_indo = train_test_split(df_en_indo, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vietnamese\n",
        "train_df, test_df_en_vi = train_test_split(df_en_vi, test_size=0.2, random_state=42)\n",
        "\n",
        "test_df = test_df_en_indo.join(test_df_en_vi[\"vi\"])\n",
        "test_df.dropna(inplace=True)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "GNyfA42HojW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwgfO6_bcA5"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Load tokenizers: basic Marian tokenizer + pretrained mBart tokenizer for both languages\n",
        "####\n",
        "\n",
        "from transformers import AutoTokenizer, MBart50TokenizerFast, MarianTokenizer\n",
        "import random, transformers\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "mbart_tokenizer_vi = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"vi_VN\")\n",
        "mbart_tokenizer_indo = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"id_ID\")\n",
        "\n",
        "marian_tokenizer_vi_en = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
        "marian_tokenizer_en_indo = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFrFyhQBbpsb"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Extend Marian tokenizer by adding tokens from target language according\n",
        "# to how mBart tokenizes sentences in the dataset\n",
        "####\n",
        "\n",
        "import transformers\n",
        "from tqdm import tqdm\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "## Add vietnamese tokens to vi-en tokenizer\n",
        "tokensZoo = []\n",
        "\n",
        "for sentence in tqdm(list(df_en_vi[\"vi\"])):\n",
        "  tokenized_sentence = mbart_tokenizer_vi(sentence)\n",
        "  for t in mbart_tokenizer_vi.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"]):\n",
        "    if t.lstrip(\"▁\") not in tokensZoo and t.lstrip(\"▁\") != \"\":\n",
        "      tokensZoo.append(t.lstrip(\"▁\"))\n",
        "    \n",
        "\n",
        "print(f\"{len(tokensZoo)} tokens to be added.\")\n",
        "print(f\"initial vocab size: {len(marian_tokenizer_vi_en)}\")\n",
        "initial_len = len(marian_tokenizer_vi_en)\n",
        "marian_tokenizer_vi_en.add_tokens(tokensZoo, special_tokens=True)\n",
        "marian_tokenizer_vi_en.add_tokens([\"<zh>\", \"<vi>\"], special_tokens=True)\n",
        "print(f\"final vocab size: {len(marian_tokenizer_vi_en)}\")\n",
        "added_tokens = len(marian_tokenizer_vi_en) - initial_len\n",
        "\n",
        "\n",
        "## Add indonesian tokens to en-indo tokenizer\n",
        "tokensZoo = []\n",
        "\n",
        "for sentence in tqdm(list(df_en_indo[\"indo\"])):\n",
        "  tokenized_sentence = mbart_tokenizer_indo(sentence)\n",
        "  for t in mbart_tokenizer_indo.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"]):\n",
        "    if t.lstrip(\"▁\") not in tokensZoo and t.lstrip(\"▁\") != \"\":\n",
        "      tokensZoo.append(t.lstrip(\"▁\"))\n",
        "    \n",
        "\n",
        "print(f\"{len(tokensZoo)} tokens to be added.\")\n",
        "print(f\"initial vocab size: {len(marian_tokenizer_en_indo)}\")\n",
        "initial_len = len(marian_tokenizer_en_indo)\n",
        "marian_tokenizer_en_indo.add_tokens(tokensZoo, special_tokens=True)\n",
        "marian_tokenizer_en_indo.add_tokens([\"<2zh>\", \"<2indo>\"], special_tokens=True)\n",
        "print(f\"final vocab size: {len(marian_tokenizer_en_indo)}\")\n",
        "added_tokens = len(marian_tokenizer_en_indo) - initial_len"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Phase\n",
        "Vietnamese -> English"
      ],
      "metadata": {
        "id": "-P6lnB54mbkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "# Load vietnamese - english model \n",
        "####\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model_vi_en = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_vi_en)"
      ],
      "metadata": {
        "id": "dsoPvNSDLu-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Tokenize sentences in Vietnamese and English (using only test set) and create first dataset\n",
        "###\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "ds_vi = Dataset.from_pandas(test_df)\n",
        "batch_size = 64\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [s for s in examples[\"vi\"]]\n",
        "    targets = [s for s in examples[\"en\"]]\n",
        "    model_inputs = marian_tokenizer_vi_en(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with marian_tokenizer_vi_en.as_target_tokenizer():\n",
        "        labels = marian_tokenizer_vi_en(targets, max_length=64, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "columns_to_return = ['input_ids', 'labels', 'attention_mask']\n",
        "ds_vi = ds_vi.map(preprocess_function, batched=True, batch_size=16)\n",
        "ds_vi.set_format(type='torch', columns=columns_to_return)\n"
      ],
      "metadata": {
        "id": "ZdxLSTdtSKqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Define trainer arguments to translate from Vietnamese to English\n",
        "###\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "  \"./\", \n",
        "  evaluation_strategy = \"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  per_device_eval_batch_size=batch_size,\n",
        "  weight_decay=0.01,\n",
        "  save_total_limit=1,\n",
        "  num_train_epochs=5,\n",
        "  predict_with_generate=True,\n",
        "  fp16=True, #CUDA purposes,\n",
        "  disable_tqdm=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(marian_tokenizer_vi_en, model=model_vi_en)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model_vi_en,\n",
        "    args,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=marian_tokenizer_vi_en,\n",
        ")"
      ],
      "metadata": {
        "id": "VtW9ZlD7k-MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Translate from Vietnamese to English and decode predicted tokens\n",
        "# to get translated sentences\n",
        "###\n",
        "\n",
        "predictions = trainer.predict(ds_vi)\n",
        "\n",
        "translated_en_sentences = []\n",
        "for p in tqdm(predictions[0]):\n",
        "  translated_en_sentences.append(marian_tokenizer_vi_en.decode(p, skip_special_tokens=True))\n",
        "\n",
        "print(translated_en_sentences[:3])"
      ],
      "metadata": {
        "id": "yrU1Xk7Vlzry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Phase\n",
        "English -> Indonesian"
      ],
      "metadata": {
        "id": "2WZh5HrQmLmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Create dataframe with English translations + Indonesian\n",
        "###\n",
        "\n",
        "test_df_en_indo = pd.DataFrame(translated_en_sentences)\n",
        "test_df_en_indo[\"indo\"] = test_df[\"indo\"].values\n",
        "test_df_en_indo = test_df_en_indo.rename(columns={0:\"en\"})\n",
        "test_df_en_indo[\"en\"] = \"<2indo> \" + test_df_en_indo[\"en\"]\n",
        "test_df_en_indo.head()"
      ],
      "metadata": {
        "id": "3fnDr1RKmIVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OOetIhRNoBq"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Tokenize the sentences in the dataset\n",
        "####\n",
        "\n",
        "from transformers import AutoTokenizer, MBart50TokenizerFast, MarianTokenizer\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import transformers    \n",
        "\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.CRITICAL)\n",
        "\n",
        "# create dataset objects\n",
        "test_ds_en_indo = Dataset.from_pandas(test_df_en_indo)\n",
        "\n",
        "max_input_length = 64\n",
        "max_target_length = 64\n",
        "batch_size = 16\n",
        "\n",
        "# To tokenize English, use a basic Marian tokenizer (the extended version\n",
        "# has some problems when tokenizing English)\n",
        "pure_marian_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
        "pure_marian_tokenizer.add_tokens([\"<2zh>\", \"<2indo>\"], special_tokens=True)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    targets = [s for s in examples[\"indo\"]]\n",
        "    inputs = [s for s in examples[\"en\"]]\n",
        "\n",
        "    model_inputs = pure_marian_tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "    # Setup the tokenizer for targets\n",
        "    with marian_tokenizer_en_indo.as_target_tokenizer():\n",
        "        labels = marian_tokenizer_en_indo(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    \n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "test_ds_en_indo = test_ds_en_indo.map(preprocess_function, batched=True, batch_size=batch_size)\n",
        "test_ds_en_indo.set_format(type='torch', columns=columns_to_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhIdjF_L59ju"
      },
      "outputs": [],
      "source": [
        "####\n",
        "# Define the function to compute the BLEU score during training\n",
        "####\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = marian_tokenizer_en_indo.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, marian_tokenizer_en_indo.pad_token_id)\n",
        "    decoded_labels = marian_tokenizer_en_indo.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    print(\"Decoded preds: \", decoded_preds[0:3])\n",
        "    print(\"Decoded labels: \", decoded_labels[0:3])\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != marian_tokenizer_en_indo.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Download the pretrained en-indo model from the hub\n",
        "###\n",
        "\n",
        "model_en_indo = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_en_indo)"
      ],
      "metadata": {
        "id": "imU4CMYhjQAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTuB-IGrV6nI"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# Define training arguments\n",
        "###\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "  \"/content/drive/MyDrive/mixed_ckp\",\n",
        "  evaluation_strategy = \"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  learning_rate=2e-5,\n",
        "  per_device_train_batch_size=batch_size,\n",
        "  per_device_eval_batch_size=batch_size,\n",
        "  weight_decay=0.01,\n",
        "  save_total_limit=1,\n",
        "  num_train_epochs=5,\n",
        "  predict_with_generate=True,\n",
        "  fp16=True, #CUDA purposes,\n",
        "  disable_tqdm=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(marian_tokenizer_en_indo, model=model_en_indo)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model_en_indo,\n",
        "    args,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=marian_tokenizer_en_indo,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "# Translate from English to Indonesian and evaluate predictions\n",
        "###\n",
        "\n",
        "print(trainer.predict(test_ds_en_indo))"
      ],
      "metadata": {
        "id": "bVorp02rlYyK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Low_resource_MT_vi_indo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}